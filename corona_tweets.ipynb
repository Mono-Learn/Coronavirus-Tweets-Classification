{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <h1>SVM Classification - SVC</h1>\n",
    "    <h3>Project: Coronavirus tweets classification</h3>\n",
    "    <h4><a href=\"https://amzenterprise.ir/\">Ali Momenzadeh</a></h5>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset on Kaggle: https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('Corona_NLP_train.csv',encoding=\"latin1\")\n",
    "test_set = pd.read_csv('Corona_NLP_test.csv',encoding=\"latin1\")\n",
    "\n",
    "train_set.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop unrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrelevant_features = [\"UserName\",\"ScreenName\",\"Location\",\"TweetAt\"]\n",
    "\n",
    "train_set.drop(unrelevant_features,inplace=True,axis=1)\n",
    "test_set.drop(unrelevant_features,inplace=True,axis=1)\n",
    "train_set.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency of each Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[\"Sentiment\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = train_set[(train_set[\"Sentiment\"] == \"Positive\") | (train_set[\"Sentiment\"] == \"Extremely Positive\")]\n",
    "positives_test = test_set[(test_set[\"Sentiment\"] == \"Positive\") | (test_set[\"Sentiment\"] == \"Extremely Positive\")]\n",
    "print(positives[\"Sentiment\"].value_counts())\n",
    "positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = train_set[(train_set[\"Sentiment\"] == \"Negative\") | (train_set[\"Sentiment\"] == \"Extremely Negative\")]\n",
    "negatives_test = test_set[(test_set[\"Sentiment\"] == \"Negative\") | (test_set[\"Sentiment\"] == \"Extremely Negative\")]\n",
    "print(negatives[\"Sentiment\"].value_counts())\n",
    "negatives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutrals = train_set[train_set[\"Sentiment\"] == \"Neutral\"]\n",
    "neutrals_test = test_set[test_set[\"Sentiment\"] == \"Neutral\"]\n",
    "print(neutrals[\"Sentiment\"].value_counts())\n",
    "neutrals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives[\"Sentiment\"] = 0 \n",
    "negatives_test[\"Sentiment\"] = 0\n",
    "\n",
    "positives[\"Sentiment\"] = 2\n",
    "positives_test[\"Sentiment\"] = 2\n",
    "\n",
    "neutrals[\"Sentiment\"] = 1\n",
    "neutrals_test[\"Sentiment\"] = 1\n",
    "\n",
    "negatives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([positives,\n",
    "                  positives_test,\n",
    "                  neutrals,\n",
    "                  neutrals_test,\n",
    "                  negatives,\n",
    "                  negatives_test\n",
    "                 ],axis=0)\n",
    "\n",
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Randomly select data points for examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in range(1,10):\n",
    "    random_ind = random.randint(0,len(data))\n",
    "    print(str(data[\"OriginalTweet\"][random_ind]),end=\"\\nLabel: \")\n",
    "    print(str(data[\"Sentiment\"][random_ind]),end=\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Frequency distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveFD = nltk.FreqDist(word for text in data[data[\"Sentiment\"] == 2][\"OriginalTweet\"] for word in text.lower().split())\n",
    "negativeFD = nltk.FreqDist(word  for text in data[data[\"Sentiment\"] == 0][\"OriginalTweet\"] for word in text.lower().split())\n",
    "neutralDF = nltk.FreqDist(word  for text in data[data[\"Sentiment\"] == 1][\"OriginalTweet\"] for word in text.lower().split())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Most used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(8,6))\n",
    "plt.title(\"Most Used Words in Positive Tweets\")\n",
    "positiveFD.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(8,6))\n",
    "plt.title(\"Most Used Words in Negative Tweets\")\n",
    "negativeFD.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(8,6))\n",
    "plt.title(\"Most Used Words in Neutral Tweets\")\n",
    "neutralDF.plot(50)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedData = []\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "swords = stopwords.words(\"english\") #In English, “the”, “is” and “and”, would easily qualify as stop words.\n",
    "for text in data[\"OriginalTweet\"]:\n",
    "    \n",
    "    # Cleaning links\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Cleaning everything except alphabetical and numerical characters\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text)\n",
    "    \n",
    "    # Tokenizing and lemmatizing\n",
    "    text = nltk.word_tokenize(text.lower())\n",
    "    text = [lemma.lemmatize(word) for word in text]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    text = [word for word in text if word not in swords]\n",
    "    \n",
    "    # Joining\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    cleanedData.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    print(cleanedData[i],end=\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/714/1*UOjWvDziH86T2MmiDpp98Q.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/cassieview/intro-nlp-wine-reviews/master/imgs/vectorchart.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "# Bag Of Words\n",
    "BOW = vectorizer.fit_transform(cleanedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(BOW,np.asarray(data[\"Sentiment\"]))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "start_time = time.time()\n",
    "\n",
    "model = SVC()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "process_time = round(end_time-start_time,2)\n",
    "print(\"Fitting SVC took {} seconds\".format(process_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy of the model is {}%\".format(accuracy_score(y_test,predictions) * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix_files/confusion_matrix_1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
